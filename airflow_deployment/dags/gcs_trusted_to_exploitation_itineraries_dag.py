import pandas as pd
import pyarrow.parquet as pq
import tempfile
import os
from airflow.providers.google.cloud.hooks.gcs import GCSHook
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from datetime import datetime
import logging
import io
import pytz
from collections import defaultdict
from airflow import DAG
from pymongo import MongoClient
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class GCSLogHandler:
    def __init__(self, bucket, gcs_hook, dag_id):
        self.bucket = bucket
        self.storage_client = gcs_hook.get_conn()
        self.dag_id = dag_id
        self._create_buffer()
        
    def _create_buffer(self):
        """Creates a new string buffer for logging."""
        self.log_buffer = io.StringIO()
        
    def write(self, message):
        """Write message to buffer, creating new buffer if closed."""
        if self.log_buffer.closed:
            self._create_buffer()
        self.log_buffer.write(message)
        
    def flush(self):
        """Flush the buffer if it's not closed."""
        if not self.log_buffer.closed:
            self.log_buffer.flush()
        
    def save_logs(self, context):
        """Save logs to GCS and reset buffer."""
        if self.log_buffer.closed:
            return
            
        log_content = self.log_buffer.getvalue()
        current_time = datetime.now(pytz.UTC)
        timestamp = current_time.strftime("%Y_%m_%d_%H_%M_%S")
        log_path = f"exploitation_zone/logs/{timestamp}.log"
        
        metadata = f"""
Task Run Metadata:
----------------
DAG ID: {self.dag_id}
Task ID: {context['task'].task_id}
Logical Date: {context['logical_date']}
Start Date: {context['task_instance'].start_date}
End Date: {current_time}
----------------
"""
        full_log_content = metadata + "\n" + log_content
        
        bucket = self.storage_client.bucket(self.bucket)
        blob = bucket.blob(log_path)
        blob.upload_from_string(full_log_content, content_type='text/plain')
        
        self.log_buffer.close()
        self._create_buffer()

def capture_airflow_logs(bucket_name, dag_id, context):
    """Capture and store Airflow logs in GCS."""
    gcs_hook = GCSHook()
    
    root_logger = logging.getLogger()
    
    gcs_handler = GCSLogHandler(bucket_name, gcs_hook, dag_id)
    stream_handler = logging.StreamHandler(gcs_handler)
    
    formatter = logging.Formatter(
        '[%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s',
        '%Y-%m-%d, %H:%M:%S UTC'
    )
    stream_handler.setFormatter(formatter)
    
    root_logger.addHandler(stream_handler)
    
    return gcs_handler

class ItineraryLoader:
    def __init__(self, mongo_uri: str, db_name: str = "itinerary_db", collection_name: str = "itineraries"):
        """
        Initialize the itinerary loader
        
        Args:
            mongo_uri: MongoDB connection URI
            db_name: Name of the MongoDB database
            collection_name: Name of the MongoDB collection
        """
        self.client = MongoClient(mongo_uri)
        self.db = self.client[db_name]
        self.collection = self.db[collection_name]
        
    def parse_filename(self, filename: str) -> dict:
        """
        Parse the filename to extract city, type, and days information
        
        Args:
            filename: Name of the itinerary file
            
        Returns:
            Dictionary containing parsed information
        """
        # Remove .txt extension and split by underscore
        parts = filename.replace('.txt', '').split('_')
        
        if len(parts) != 4:
            raise ValueError(f"Invalid filename format: {filename}")
            
        return {
            'city': parts[1],
            'days': int(parts[2]),
            'type': parts[3]
        }
        
    def create_indexes(self):
        """Create indexes for efficient querying"""
        # Create compound index on city, type, and days
        self.collection.create_index([
            ('city', 1),
            ('type', 1),
            ('days', 1)
        ])
        logging.info("Created indexes for efficient querying")
    
    def close(self):
        """Close MongoDB connection"""
        self.client.close()
        logging.info("Closed MongoDB connection")

def process_itineraries_to_mongodb(
    bucket_name,
    **context
):
    gcs_hook = GCSHook()
    storage_client = gcs_hook.get_conn()
    bucket = storage_client.bucket(bucket_name)
    gcs_handler = capture_airflow_logs(bucket_name, context['dag'].dag_id, context)
    logger = logging.getLogger(f"airflow.task.{context['task'].task_id}")

    # Get MongoDB connection string from environment variable
    mongo_uri = os.getenv('MONGODB_CONNECTION_STRING')
    
    # Initialize MongoDB loader
    loader = ItineraryLoader(mongo_uri)

    try:
        cities = ['barcelona', 'madrid', 'paris']
        today = datetime.now().strftime("%Y_%m_%d")
        logger.info(f"Processing itineraries for date: {today}")

        for city in cities:
            logger.info(f"Processing city: {city}")
            parquet_blob_path = f"trusted_zone/itineraries/{city}/{today}_itineraries.parquet"
            blob = bucket.blob(parquet_blob_path)
            if not blob.exists():
                logger.warning(f"No Parquet file found for {city} on {today}")
                continue

            # Download Parquet to temp file
            with tempfile.NamedTemporaryFile(delete=False, suffix='.parquet') as tmp_file:
                blob.download_to_filename(tmp_file.name)
                tmp_file_path = tmp_file.name

            try:
                df = pd.read_parquet(tmp_file_path, engine='pyarrow')
                logger.info(f"Loaded {len(df)} itineraries for {city} from Parquet")

                # Group by user_persona
                for persona, persona_df in df.groupby('user_persona'):
                    persona_data = persona_df.to_dict(orient='records')
                    
                    # Create document structure
                    document = {
                        'city': city,
                        'type': persona,
                        'days': int(persona_df['day_number'].iloc[0]),
                        'itinerary': persona_data,
                    }
                    
                    # Insert into MongoDB
                    result = loader.collection.insert_one(document)
                    logger.info(f"Loaded itinerary for {city} - {persona} ({document['days']} days)")

            finally:
                os.remove(tmp_file_path)

    except Exception as e:
        logger.error(f"Error in process_itineraries_to_mongodb: {str(e)}")
        raise
    finally:
        # Create indexes
        loader.create_indexes()
        
        # Close MongoDB connection
        loader.close()
        
        # Save logs to GCS
        gcs_handler.save_logs(context)

with DAG(
    'gcs_trusted_to_exploitation_itineraries',
    start_date=datetime(2025, 5, 11),
    description='DAG to process itinerary data from trusted zone into MongoDB in exploitation zone.',
    tags=['gcs', 'mongodb', 'itineraries', 'exploitation'],
    schedule='@daily',
    catchup=False
) as dag:
    gcs_bucket_name = "bdm-project"

    process_itineraries_task = PythonOperator(
        task_id='process_itineraries_to_mongodb',
        python_callable=process_itineraries_to_mongodb,
        op_kwargs={
            'bucket_name': gcs_bucket_name
        }
    )