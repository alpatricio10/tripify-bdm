from airflow import DAG
from airflow.providers.google.cloud.hooks.gcs import GCSHook
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import duckdb
import os
import logging
import io
import pytz
import tempfile
from google.cloud import storage

class GCSLogHandler:
    def __init__(self, bucket, gcs_hook, dag_id):
        self.bucket = bucket
        self.storage_client = gcs_hook.get_conn()
        self.dag_id = dag_id
        self.log_buffer = io.StringIO()
        
    def write(self, message):
        self.log_buffer.write(message)
        
    def flush(self):
        pass
        
    def save_logs(self, context):
        log_content = self.log_buffer.getvalue()
        current_time = datetime.now(pytz.UTC)
        timestamp = current_time.strftime("%Y_%m_%d_%H_%M_%S")
        log_path = f"exploitation_zone/logs/{timestamp}.log"
        
        metadata = f"""
Task Run Metadata:
----------------
DAG ID: {self.dag_id}
Task ID: {context['task'].task_id}
Logical Date: {context['logical_date']}
Start Date: {context['task_instance'].start_date}
End Date: {current_time}
----------------
"""
        full_log_content = metadata + "\n" + log_content
        
        bucket = self.storage_client.bucket(self.bucket)
        blob = bucket.blob(log_path)
        blob.upload_from_string(full_log_content, content_type='text/plain')
        self.log_buffer.close()

def capture_airflow_logs(bucket_name, dag_id, context):
    """Capture and store Airflow logs in GCS."""
    gcs_hook = GCSHook()
    
    root_logger = logging.getLogger()
    
    gcs_handler = GCSLogHandler(bucket_name, gcs_hook, dag_id)
    stream_handler = logging.StreamHandler(gcs_handler)
    
    formatter = logging.Formatter(
        '[%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s',
        '%Y-%m-%d, %H:%M:%S UTC'
    )
    stream_handler.setFormatter(formatter)
    
    root_logger.addHandler(stream_handler)
    
    return gcs_handler

def process_transport_data(bucket_name, **context):
    """Process transport data from trusted zone to exploitation zone using DuckDB."""
    gcs_hook = GCSHook()
    storage_client = gcs_hook.get_conn()
    bucket = storage_client.bucket(bucket_name)
    
    # Setup logging to GCS
    gcs_handler = capture_airflow_logs(bucket_name, context['dag'].dag_id, context)
    logger = logging.getLogger(f"airflow.task.{context['task'].task_id}")
    
    try:
        # Get today's date in the format used in GCS
        today = datetime.now().strftime("%Y%m%d")
        logger.info(f"Starting processing for date: {today}")
        
        # Define transport types
        transport_types = ['air', 'bus', 'train']
        
        # Create a temporary directory for DuckDB file
        with tempfile.TemporaryDirectory() as temp_dir:
            duckdb_path = os.path.join(temp_dir, 'transport_data.duckdb')
            
            # Initialize DuckDB connection
            conn = duckdb.connect(duckdb_path)
            
            # Create tables if they don't exist
            conn.execute("""
                CREATE TABLE IF NOT EXISTS routes (
                    route_id INTEGER PRIMARY KEY,
                    departure_city VARCHAR,
                    departure_country VARCHAR,
                    departure_station VARCHAR,
                    arrival_city VARCHAR,
                    arrival_country VARCHAR,
                    arrival_station VARCHAR,
                    transport_type VARCHAR
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS dates (
                    date_id INTEGER PRIMARY KEY,
                    full_date DATE,
                    year INTEGER,
                    month INTEGER,
                    day INTEGER,
                    quarter INTEGER,
                    day_of_week INTEGER,
                    is_weekend BOOLEAN,
                    is_holiday BOOLEAN
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS companies (
                    company_id INTEGER PRIMARY KEY,
                    company_name VARCHAR,
                    transport_type VARCHAR
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS times (
                    time_id INTEGER PRIMARY KEY,
                    hour INTEGER,
                    minute INTEGER,
                    time_of_day VARCHAR,
                    is_peak_hour BOOLEAN,
                    is_business_hour BOOLEAN
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS transport_fares (
                    fare_id INTEGER PRIMARY KEY,
                    route_id INTEGER,
                    date_id INTEGER,
                    company_id INTEGER,
                    time_id INTEGER,
                    price DECIMAL(10,2),
                    duration_minutes INTEGER,
                    departure_timestamp TIMESTAMP,
                    arrival_timestamp TIMESTAMP,
                    valid_from TIMESTAMP,
                    valid_to TIMESTAMP,
                    is_current BOOLEAN,
                    created_at TIMESTAMP,
                    FOREIGN KEY (route_id) REFERENCES routes(route_id),
                    FOREIGN KEY (date_id) REFERENCES dates(date_id),
                    FOREIGN KEY (company_id) REFERENCES companies(company_id),
                    FOREIGN KEY (time_id) REFERENCES times(time_id)
                )
            """)
            
            # Process each transport type
            for transport_type in transport_types:
                logger.info(f"Processing {transport_type} data")
                
                # Define the base prefix for the transport type
                base_prefix = f'trusted_zone/transport/{transport_type}fare/{today}/'
                logger.info(f"Looking for data in: {base_prefix}")
                
                # First, find all .parquet folders
                parquet_folders = set()
                blobs = list(bucket.list_blobs(prefix=base_prefix))
                logger.info(f"Found {len(blobs)} total blobs under {base_prefix}")
                
                # Extract unique folder names that end with .parquet
                for blob in blobs:
                    logger.info(f"Examining blob: {blob.name}")
                    path_parts = blob.name.split('/')
                    if len(path_parts) > 2:  # We need at least 3 parts: trusted_zone/transport/typefare/date/folder
                        folder = '/'.join(path_parts[:-1]) + '/'  # Get everything except the last part
                        if folder.endswith('.parquet/'):
                            parquet_folders.add(folder)
                            logger.info(f"Found .parquet folder: {folder}")
                
                if not parquet_folders:
                    logger.info(f"No .parquet folders found for {transport_type} on {today}")
                    continue
                
                logger.info(f"Found {len(parquet_folders)} .parquet folders for {transport_type}")
                
                # Process each .parquet folder
                for parquet_folder in parquet_folders:
                    logger.info(f"Processing folder: {parquet_folder}")
                    
                    # Look for files in _success and _committed folders
                    status_folders = ['_success', '_committed']
                    parquet_files = []
                    
                    for status in status_folders:
                        status_prefix = f'{parquet_folder}{status}/'
                        logger.info(f"Looking in status folder: {status_prefix}")
                        status_blobs = list(bucket.list_blobs(prefix=status_prefix))
                        logger.info(f"Found {len(status_blobs)} blobs in {status_prefix}")
                        
                        # Filter for part files
                        part_files = [b for b in status_blobs if b.name.startswith(f'{status_prefix}part-')]
                        parquet_files.extend(part_files)
                        logger.info(f"Found {len(part_files)} part files in {status_prefix}")
                    
                    if not parquet_files:
                        logger.info(f"No part files found in {parquet_folder}")
                        continue
                    
                    logger.info(f"Found {len(parquet_files)} part files in {parquet_folder}")
                    
                    # Process each part file
                    for blob in parquet_files:
                        logger.info(f"Processing file: {blob.name}")
                        
                        # Download the file to a temporary location
                        temp_file = os.path.join(temp_dir, f'{transport_type}_{os.path.basename(blob.name)}')
                        blob.download_to_filename(temp_file)
                        
                        # Read the parquet file into DuckDB
                        conn.execute(f"""
                            CREATE TABLE IF NOT EXISTS temp_{transport_type} AS 
                            SELECT * FROM read_parquet('{temp_file}')
                        """)
                        
                        # Process the data and insert into the data warehouse tables
                        conn.execute(f"""
                            INSERT INTO routes (
                                departure_city, departure_country, departure_station,
                                arrival_city, arrival_country, arrival_station,
                                transport_type
                            )
                            SELECT DISTINCT
                                Departure, Departure_Country, Departure_Station,
                                Arrival, Arrival_Country, Arrival_Station,
                                Type
                            FROM temp_{transport_type}
                            ON CONFLICT DO NOTHING
                        """)
                        
                        # Insert into fact table
                        conn.execute(f"""
                            INSERT INTO transport_fares (
                                route_id, date_id, company_id, time_id,
                                price, duration_minutes, departure_timestamp,
                                arrival_timestamp, valid_from, is_current, created_at
                            )
                            SELECT 
                                r.route_id,
                                d.date_id,
                                c.company_id,
                                t.time_id,
                                Price,
                                duration_minutes,
                                departure_timestamp,
                                arrival_timestamp,
                                CURRENT_TIMESTAMP,
                                TRUE,
                                CURRENT_TIMESTAMP
                            FROM temp_{transport_type} temp
                            JOIN routes r ON 
                                temp.Departure = r.departure_city AND
                                temp.Arrival = r.arrival_city AND
                                temp.Type = r.transport_type
                            JOIN dates d ON DATE(temp.departure_timestamp) = d.full_date
                            JOIN companies c ON temp.Company = c.company_name
                            JOIN times t ON 
                                EXTRACT(HOUR FROM temp.departure_timestamp) = t.hour AND
                                EXTRACT(MINUTE FROM temp.departure_timestamp) = t.minute
                        """)
                        
                        # Clean up temporary table
                        conn.execute(f"DROP TABLE temp_{transport_type}")
                        
                        # Remove temporary file
                        os.remove(temp_file)
            
            # Update previous records to set is_current = FALSE
            conn.execute("""
                UPDATE transport_fares
                SET is_current = FALSE,
                    valid_to = CURRENT_TIMESTAMP
                WHERE is_current = TRUE
                AND (route_id, company_id, departure_timestamp) IN (
                    SELECT route_id, company_id, departure_timestamp
                    FROM transport_fares
                    WHERE is_current = TRUE
                    GROUP BY route_id, company_id, departure_timestamp
                    HAVING COUNT(*) > 1
                )
            """)
            
            # Upload the DuckDB file to exploitation zone
            exploitation_path = f'exploitation_zone/transport/transport_data.duckdb'
            blob = bucket.blob(exploitation_path)
            blob.upload_from_filename(duckdb_path)
            
            logger.info("Successfully processed and uploaded transport data")
            
    except Exception as e:
        logger.error(f"Error processing transport data: {str(e)}")
        raise
    finally:
        if 'conn' in locals():
            conn.close()

# Define the DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'databricks_trusted_to_exploitation_transport',
    default_args=default_args,
    description='Process transport data from trusted zone to exploitation zone',
    schedule='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
)

# GCS bucket name
BUCKET_NAME = 'bdm-project'

# Create the main processing task
process_task = PythonOperator(
    task_id='process_transport_data',
    python_callable=process_transport_data,
    op_kwargs={'bucket_name': BUCKET_NAME},
    dag=dag
) 