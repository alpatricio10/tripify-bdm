from airflow import DAG
from airflow.providers.google.cloud.hooks.gcs import GCSHook
from airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import logging
import io
import pytz
from airflow.models import Variable

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GCSLogHandler:
    def __init__(self, bucket, gcs_hook, dag_id):
        self.bucket = bucket
        self.storage_client = gcs_hook.get_conn()
        self.dag_id = dag_id
        self.log_buffer = io.StringIO()
        
    def write(self, message):
        self.log_buffer.write(message)
        
    def flush(self):
        pass
        
    def save_logs(self, context):
        log_content = self.log_buffer.getvalue()
        current_time = datetime.now(pytz.UTC)
        timestamp = current_time.strftime("%Y_%m_%d_%H_%M_%S")
        log_path = f"exploitation_zone/logs/{timestamp}.log"
        
        metadata = f"""
Task Run Metadata:
----------------
DAG ID: {self.dag_id}
Task ID: {context['task'].task_id}
Logical Date: {context['logical_date']}
Start Date: {context['task_instance'].start_date}
End Date: {current_time}
----------------
"""
        full_log_content = metadata + "\n" + log_content
        
        bucket = self.storage_client.bucket(self.bucket)
        blob = bucket.blob(log_path)
        blob.upload_from_string(full_log_content, content_type='text/plain')
        self.log_buffer.close()

def capture_airflow_logs(bucket_name, dag_id, context):
    """Capture and store Airflow logs in GCS."""
    gcs_hook = GCSHook()
    
    root_logger = logging.getLogger()
    
    gcs_handler = GCSLogHandler(bucket_name, gcs_hook, dag_id)
    stream_handler = logging.StreamHandler(gcs_handler)
    
    formatter = logging.Formatter(
        '[%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s',
        '%Y-%m-%d, %H:%M:%S UTC'
    )
    stream_handler.setFormatter(formatter)
    
    root_logger.addHandler(stream_handler)
    
    return gcs_handler

def process_users_data(bucket_name, **context):
    """Process users data using Databricks"""
    gcs_hook = GCSHook()
    execution_date = context['logical_date']
    date_str = execution_date.strftime('%Y-%m-%d')
    
    logging.info(f"Starting processing for users data")
    logging.info(f"Execution date: {execution_date}")
    
    # Define GCS paths
    trusted_path = f'gs://{bucket_name}/trusted_zone/users/{date_str}/'
    exploitation_path = f'gs://{bucket_name}/exploitation_zone/users/{date_str}/neo4j/'
    
    # Process files in specific order to ensure proper data relationships
    files_to_process = [
        ('users.parquet', 'users'),  # Process users first
        ('posts.parquet', 'posts'),  # Then posts
        ('likes.parquet', 'likes')   # Finally likes
    ]
    
    for file_name, file_type in files_to_process:
        input_path = f'{trusted_path}{file_name}'
        logging.info(f"Checking file: {input_path}")
        
        # Check if file exists in GCS
        try:
            bucket = gcs_hook.get_bucket(bucket_name)
            blob = bucket.blob(f'trusted_zone/users/{date_str}/{file_name}')
            if not blob.exists():
                logging.warning(f"File {input_path} does not exist. Skipping processing.")
                continue
        except Exception as e:
            logging.error(f"Error checking file existence: {str(e)}")
            all_files_exist = False
    
    if not all_files_exist:
        logging.error("Not all required files exist. Skipping processing.")
        return
    
    logging.info("All required files exist. Starting processing...")
    
    # Create Databricks task with all file paths
    databricks_task = DatabricksSubmitRunOperator(
        task_id='process_all_files',
        databricks_conn_id='databricks_default',
        existing_cluster_id='0525-225010-u4og1d5d',  
        notebook_task={
            'notebook_path': '/Workspace/Repos/travelwithtripify@gmail.com/tripify/process_users',
            'base_parameters': {
                'users_path': input_files['users'],
                'posts_path': input_files['posts'],
                'likes_path': input_files['likes'],
                'output_path': exploitation_path
            }
        },
        dag=dag
    )
    
    # Execute the task
    try:
        databricks_task.execute(context)
        logging.info("Successfully processed all files")
    except Exception as e:
        logging.error(f"Error processing files: {str(e)}")
        raise

# Define the DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'databricks_trusted_to_exploitation_users',
    default_args=default_args,
    description='Process users data from trusted zone to exploitation zone using Databricks',
    schedule='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
)

# GCS bucket name
BUCKET_NAME = 'bdm-project'

# Create the main processing task
process_task = PythonOperator(
    task_id='process_users_data',
    python_callable=process_users_data,
    op_kwargs={'bucket_name': BUCKET_NAME},
    dag=dag
) 