import pandas as pd
import numpy as np
import duckdb
import os
import logging
import tempfile
from airflow.providers.google.cloud.hooks.gcs import GCSHook
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from datetime import datetime
import pytz
from airflow import DAG
import io

class GCSLogHandler:
    def __init__(self, bucket, gcs_hook, dag_id):
        self.bucket = bucket
        self.storage_client = gcs_hook.get_conn()
        self.dag_id = dag_id
        self.log_buffer = io.StringIO()
    def write(self, message):
        self.log_buffer.write(message)
    def flush(self):
        pass
    def save_logs(self, context):
        log_content = self.log_buffer.getvalue()
        current_time = datetime.now(pytz.UTC)
        timestamp = current_time.strftime("%Y_%m_%d_%H_%M_%S")
        log_path = f"exploitation_zone/logs/{timestamp}.log"
        metadata = f"""
Task Run Metadata:
----------------
DAG ID: {self.dag_id}
Task ID: {context['task'].task_id}
Logical Date: {context['logical_date']}
Start Date: {context['task_instance'].start_date}
End Date: {current_time}
----------------
"""
        full_log_content = metadata + "\n" + log_content
        bucket = self.storage_client.bucket(self.bucket)
        blob = bucket.blob(log_path)
        blob.upload_from_string(full_log_content, content_type='text/plain')
        self.log_buffer.close()

def capture_airflow_logs(bucket_name, dag_id, context):
    gcs_hook = GCSHook()
    root_logger = logging.getLogger()
    gcs_handler = GCSLogHandler(bucket_name, gcs_hook, dag_id)
    stream_handler = logging.StreamHandler(gcs_handler)
    formatter = logging.Formatter(
        '[%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s',
        '%Y-%m-%d, %H:%M:%S UTC'
    )
    stream_handler.setFormatter(formatter)
    root_logger.addHandler(stream_handler)
    return gcs_handler

def process_events_to_exploitation(
    bucket_name,
    **context
):
    gcs_hook = GCSHook()
    storage_client = gcs_hook.get_conn()
    bucket = storage_client.bucket(bucket_name)
    
    gcs_handler = capture_airflow_logs(bucket_name, context['dag'].dag_id, context)
    logger = logging.getLogger(f"airflow.task.{context['task'].task_id}")
    try:
        today = datetime.now().strftime("%Y_%m_%d")
        cities = ['barcelona', 'madrid', 'paris']
        city_dfs = []
        temp_files = []
        for city in cities:
            logger.info(f"Processing city: {city}")
            # Find the processed parquet file in GCS
            prefix = f"trusted_zone/events/{city}/{today}/"
            blobs = list(bucket.list_blobs(prefix=prefix))
            parquet_blobs = [b for b in blobs if b.name.endswith('_processed.parquet')]
            if not parquet_blobs:
                logger.warning(f"No processed Parquet file found for {city} on {today}")
                continue
            # Use the first found (should only be one per city per day)
            blob = parquet_blobs[0]
            with tempfile.NamedTemporaryFile(delete=False, suffix='.parquet') as tmp_file:
                blob.download_to_filename(tmp_file.name)
                temp_files.append(tmp_file.name)
                df = pd.read_parquet(tmp_file.name, engine='pyarrow')
            logger.info(f"Loaded {len(df)} records for {city} from Parquet")
            df['city_name'] = city.capitalize()
            if city == 'barcelona' and 'is_free' in df.columns:
                df['is_free'] = np.random.choice(['Paid', 'Free'], size=len(df['is_free']), p=[0.42, 0.58])
            city_dfs.append(df)
        if not city_dfs:
            logger.warning("No city data loaded, skipping exploitation processing.")
            return
        events_all_df = pd.concat(city_dfs, ignore_index=True)
        logger.info(f"Total events after concatenation: {len(events_all_df)}")
        # Connect to DuckDB
        db_path = '/tmp/events_exploitation.duckdb'
        if os.path.exists(db_path):
            os.remove(db_path)
        conn = duckdb.connect(db_path)
        conn.register('events_all_df', events_all_df)
        # --- Dimensional Model Logic ---
        # Location dimension
        conn.execute("DROP TABLE IF EXISTS dim_location")
        conn.execute("""
        CREATE TABLE dim_location AS
        WITH ranked_locations AS (
            SELECT *,
                ROW_NUMBER() OVER (
                    PARTITION BY address, city_name
                    ORDER BY 
                        venue,
                        address_neighbourhood,
                        address_district,
                        zip_code,
                        latitude,
                        longitude
                ) AS rn
            FROM events_all_df
            WHERE address IS NOT NULL AND city_name IS NOT NULL
        )
        SELECT 
            ROW_NUMBER() OVER () AS location_id,
            venue,
            address,
            address_neighbourhood,
            address_district,
            zip_code,
            city_name AS city,
            country,
            latitude,
            longitude,
            x_coordinate,
            y_coordinate
        FROM ranked_locations
        WHERE rn = 1
        """)
        # Category dimension
        conn.execute("DROP TABLE IF EXISTS dim_category")
        conn.execute("""
        CREATE TABLE dim_category AS
        SELECT 
            ROW_NUMBER() OVER (ORDER BY category) AS category_id,
            category AS category_name
        FROM (
            SELECT DISTINCT category
            FROM events_all_df
            WHERE category IS NOT NULL
        ) AS distinct_categories
        """)
        # Organization dimension
        conn.execute("DROP TABLE IF EXISTS dim_organization")
        conn.execute("""
        CREATE TABLE dim_organization AS
        WITH ranked_orgs AS (
            SELECT
                ROW_NUMBER() OVER (
                    PARTITION BY organisation
                    ORDER BY organisation
                ) AS rn,
                organisation,
                contact_phone,
                contact_email,
                linkedin_handle,
                instagram_handle,
                facebook_handle,
                twitter_handle
            FROM (
                SELECT DISTINCT
                    organisation,
                    contact_phone,
                    contact_email,
                    linkedin_handle,
                    instagram_handle,
                    facebook_handle,
                    twitter_handle
                FROM events_all_df
                WHERE organisation IS NOT NULL
            ) AS distinct_organizations
        )
        SELECT
            ROW_NUMBER() OVER (ORDER BY organisation) AS organization_id,
            organisation AS organization_name,
            contact_phone,
            contact_email,
            linkedin_handle,
            instagram_handle,
            facebook_handle,
            twitter_handle
        FROM ranked_orgs
        WHERE rn = 1
        """)
        # Audience dimension
        conn.execute("DROP TABLE IF EXISTS dim_audience")
        conn.execute("""
        CREATE TABLE dim_audience AS
        SELECT 
            ROW_NUMBER() OVER (ORDER BY audience) AS audience_id,
            audience AS audience_type
        FROM (
            SELECT DISTINCT audience
            FROM events_all_df
            WHERE audience IS NOT NULL
        ) AS distinct_audiences
        """)
        # Date dimension
        conn.execute("DROP TABLE IF EXISTS dim_date")
        date_bounds = conn.execute("""
            SELECT 
                MIN(CAST(start_date AS DATE)) AS min_date, 
                MAX(CAST(COALESCE(end_date, start_date) AS DATE)) AS max_date
            FROM events_all_df
            WHERE start_date IS NOT NULL
        """).fetchone()
        min_date = date_bounds[0]
        max_date = date_bounds[1]
        if min_date is None or max_date is None:
            logger.warning("No valid dates found in events, skipping date dimension.")
            return
        day_count = (max_date - min_date).days + 1 + 30
        conn.execute(f"""
        DROP TABLE IF EXISTS dim_date;
        CREATE TABLE dim_date AS
        WITH date_range AS (
            SELECT 
                DATE '{min_date}' + CAST(i AS INTEGER) AS calendar_date
            FROM range({day_count}) AS t(i)
        )
        SELECT
            ROW_NUMBER() OVER (ORDER BY calendar_date) AS date_id,
            calendar_date AS full_date,
            EXTRACT(YEAR FROM calendar_date) AS year,
            EXTRACT(MONTH FROM calendar_date) AS month,
            EXTRACT(DAY FROM calendar_date) AS day,
            EXTRACT(DOW FROM calendar_date) AS day_of_week,
            CASE EXTRACT(DOW FROM calendar_date)
                WHEN 0 THEN 'Sunday'
                WHEN 1 THEN 'Monday'
                WHEN 2 THEN 'Tuesday'
                WHEN 3 THEN 'Wednesday'
                WHEN 4 THEN 'Thursday'
                WHEN 5 THEN 'Friday'
                WHEN 6 THEN 'Saturday'
            END AS day_name,
            EXTRACT(QUARTER FROM calendar_date) AS quarter,
            CASE 
                WHEN EXTRACT(MONTH FROM calendar_date) IN (12, 1, 2) THEN 'Winter'
                WHEN EXTRACT(MONTH FROM calendar_date) IN (3, 4, 5) THEN 'Spring'
                WHEN EXTRACT(MONTH FROM calendar_date) IN (6, 7, 8) THEN 'Summer'
                ELSE 'Fall'
            END AS season
        FROM date_range
        """)
        # Fact table
        conn.execute("DROP TABLE IF EXISTS fact_events")
        conn.execute("""
        CREATE TABLE fact_events AS
        SELECT 
            e.id AS event_id,
            e.name AS event_name,
            e.description,
            e.activity,
            e.is_free,
            e.price_details,
            e.time,
            e.excluded_days,
            e.website,
            sd.date_id AS start_date_id,
            ed.date_id AS end_date_id,
            l.location_id,
            c.category_id,
            o.organization_id,
            a.audience_id,
            e.city_name
        FROM events_all_df e
        LEFT JOIN dim_date sd ON CAST(e.start_date AS DATE) = sd.full_date
        LEFT JOIN dim_date ed ON CAST(e.end_date AS DATE) = ed.full_date
        LEFT JOIN dim_location l ON 
            e.address = l.address AND
            e.city_name = l.city
        LEFT JOIN dim_category c ON e.category = c.category_name
        LEFT JOIN dim_organization o ON e.organisation = o.organization_name
        LEFT JOIN dim_audience a ON e.audience = a.audience_type
        """)
        # Save full raw dataset
        conn.execute("DROP TABLE IF EXISTS city_events")
        conn.execute("CREATE TABLE city_events AS SELECT * FROM events_all_df")
        logger.info("Exploitation zone tables created in DuckDB.")
        # Upload DuckDB file to GCS
        try:
            gcs_duckdb_path = f"exploitation_zone/events/{today}/events_exploitation.duckdb"
            duckdb_blob = bucket.blob(gcs_duckdb_path)
            duckdb_blob.upload_from_filename(db_path)
            logger.info(f"Uploaded DuckDB file to GCS: {gcs_duckdb_path}")
        except Exception as e:
            logger.error(f"Failed to upload DuckDB file to GCS: {str(e)}")
        finally:
            if os.path.exists(db_path):
                os.remove(db_path)
                logger.info(f"Cleaned up local DuckDB file: {db_path}")
        # Clean up temp files
        for f in temp_files:
            if os.path.exists(f):
                os.remove(f)
        logger.info("Temporary files cleaned up.")
    except Exception as e:
        logger.error(f"Pipeline failed: {str(e)}")
        raise
    finally:
        gcs_handler.save_logs(context)

with DAG(
    'gcs_trusted_to_exploitation_events',
    start_date=datetime(2025, 5, 11),
    description='DAG to process event data from trusted zone into exploitation zone DuckDB.',
    tags=['gcs', 'duckdb', 'events', 'exploitation'],
    schedule='@daily',
    catchup=False
) as dag:
    gcs_bucket_name = Variable.get("events_gcs_bucket_name", default_var="bdm-project")
    process_events_task = PythonOperator(
        task_id='process_events_to_exploitation',
        python_callable=process_events_to_exploitation,
        op_kwargs={
            'bucket_name': gcs_bucket_name
        }
    ) 