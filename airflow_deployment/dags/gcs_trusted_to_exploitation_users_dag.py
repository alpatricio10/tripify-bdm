from airflow import DAG
from airflow.providers.google.cloud.hooks.gcs import GCSHook
from airflow.operators.python import PythonOperator
from datetime import datetime
import pandas as pd
import os
import json
import logging
from pathlib import Path
import subprocess
import io
import pytz
from airflow.models import Variable
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode, array, lit, struct, from_json, split, current_timestamp, to_json
from pyspark.sql.types import ArrayType, StructType, StructField, StringType, IntegerType

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GCSLogHandler:
    def __init__(self, bucket, gcs_hook, dag_id):
        self.bucket = bucket
        self.storage_client = gcs_hook.get_conn()
        self.dag_id = dag_id
        self.log_buffer = io.StringIO()
    def write(self, message):
        self.log_buffer.write(message)
    def flush(self):
        pass
    def save_logs(self, context):
        log_content = self.log_buffer.getvalue()
        current_time = datetime.now(pytz.UTC)
        timestamp = current_time.strftime("%Y_%m_%d_%H_%M_%S")
        log_path = f"exploitation_zone/logs/{timestamp}.log"
        metadata = f"""
Task Run Metadata:
----------------
DAG ID: {self.dag_id}
Task ID: {context['task'].task_id}
Logical Date: {context['logical_date']}
Start Date: {context['task_instance'].start_date}
End Date: {current_time}
----------------
"""
        full_log_content = metadata + "\n" + log_content
        bucket = self.storage_client.bucket(self.bucket)
        blob = bucket.blob(log_path)
        blob.upload_from_string(full_log_content, content_type='text/plain')
        self.log_buffer.close()

def capture_airflow_logs(bucket_name, dag_id, context):
    gcs_hook = GCSHook()
    root_logger = logging.getLogger()
    gcs_handler = GCSLogHandler(bucket_name, gcs_hook, dag_id)
    stream_handler = logging.StreamHandler(gcs_handler)
    formatter = logging.Formatter(
        '[%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s',
        '%Y-%m-%d, %H:%M:%S UTC'
    )
    stream_handler.setFormatter(formatter)
    root_logger.addHandler(stream_handler)
    return gcs_handler

class Neo4jFileGenerator:
    def __init__(self, spark):
        """Initialize with Spark session"""
        self.spark = spark
        
    def process_in_chunks(self, df, chunk_size=1000):
        """Process DataFrame in chunks to avoid memory issues"""
        total_rows = df.count()
        for i in range(0, total_rows, chunk_size):
            yield df.limit(chunk_size).offset(i)
    
    def generate_node_files(self, users_df, posts_df):
        """Generate node files for Neo4j import"""
        # Convert pandas DataFrames to Spark DataFrames
        users_spark = self.spark.createDataFrame(users_df)
        posts_spark = self.spark.createDataFrame(posts_df)
        
        # Define schema for daily_plans
        activity_schema = StructType([
            StructField("time", StringType(), True),
            StructField("activity", StringType(), True),
            StructField("duration", StringType(), True),
            StructField("location", StringType(), True),
            StructField("description", StringType(), True)
        ])
        
        day_schema = StructType([
            StructField("day", IntegerType(), True),
            StructField("activities", ArrayType(activity_schema), True)
        ])
        
        daily_plans_schema = ArrayType(day_schema)
        
        # Parse daily_plans string to array
        posts_spark = posts_spark.withColumn(
            "daily_plans_parsed",
            from_json(col("daily_plans"), daily_plans_schema)
        )
        
        # Generate User nodes
        user_nodes = users_spark.select(
            col("user_id").alias("id"),
            lit("User").alias("labels"),
            struct(
                col("user_id"),
                col("name"),
                col("email"),
                col("username"),
                col("bio"),
                col("join_date"),
                col("location"),
                col("profile_picture")
            ).alias("properties")
        )
        
        # Generate City nodes
        cities = ["Paris", "Rome", "Barcelona", "Madrid"]
        city_nodes = self.spark.createDataFrame([
            (city, "City", {"name": city})
            for city in cities
        ], ["id", "labels", "properties"])
        
        # Generate Activity nodes in chunks
        activity_nodes = None
        for chunk in self.process_in_chunks(posts_spark):
            chunk_activities = chunk.select(
                explode("daily_plans_parsed").alias("day")
            ).select(
                explode("day.activities").alias("activity")
            ).select(
                col("activity.activity").alias("id"),
                lit("Activity").alias("labels"),
                struct(
                    col("activity.activity").alias("name"),
                    col("activity.description")
                ).alias("properties")
            ).distinct()
            
            if activity_nodes is None:
                activity_nodes = chunk_activities
            else:
                activity_nodes = activity_nodes.union(chunk_activities).distinct()
        
        # Generate Post nodes
        post_nodes = posts_spark.select(
            col("post_id").alias("id"),
            lit("Post").alias("labels"),
            struct(
                col("post_id"),
                col("title"),
                col("description"),
                col("start_date"),
                col("end_date"),
                col("total_cost"),
                col("created_at"),
                col("likes_count"),
                col("comments_count")
            ).alias("properties")
        )
        
        return {
            "user_nodes": user_nodes,
            "city_nodes": city_nodes,
            "activity_nodes": activity_nodes,
            "post_nodes": post_nodes
        }
    
    def generate_edge_files(self, users_df, posts_df, likes_df):
        """Generate edge files for Neo4j import"""
        # Convert pandas DataFrames to Spark DataFrames
        users_spark = self.spark.createDataFrame(users_df)
        posts_spark = self.spark.createDataFrame(posts_df)
        likes_spark = self.spark.createDataFrame(likes_df)
        
        # Parse favorite_cities string to array and clean city names
        users_spark = users_spark.withColumn(
            "favorite_cities_parsed",
            split(col("favorite_cities"), ",")
        ).withColumn(
            "favorite_cities_parsed",
            expr("transform(favorite_cities_parsed, x -> trim(regexp_replace(x, '^\\[\\'|\\'\\]$', '')))")
        )
        
        # Define schema for daily_plans
        activity_schema = StructType([
            StructField("time", StringType(), True),
            StructField("activity", StringType(), True),
            StructField("duration", StringType(), True),
            StructField("location", StringType(), True),
            StructField("description", StringType(), True)
        ])
        
        day_schema = StructType([
            StructField("day", IntegerType(), True),
            StructField("activities", ArrayType(activity_schema), True)
        ])
        
        daily_plans_schema = ArrayType(day_schema)
        
        # Parse daily_plans string to array
        posts_spark = posts_spark.withColumn(
            "daily_plans_parsed",
            from_json(col("daily_plans"), daily_plans_schema)
        )
        
        # Generate FAVORITE relationships
        favorite_edges = users_spark.select(
            col("user_id").alias("source"),
            explode("favorite_cities_parsed").alias("target"),
            lit("FAVORITE").alias("type"),
            struct(
                lit(current_timestamp()).alias("created_at")
            ).alias("properties")
        )
        
        # Generate CREATED relationships
        created_edges = posts_spark.select(
            col("user_id").alias("source"),
            col("post_id").alias("target"),
            lit("CREATED").alias("type"),
            struct(
                lit(current_timestamp()).alias("created_at")
            ).alias("properties")
        )
        
        # Generate IN_CITY relationships
        in_city_edges = posts_spark.select(
            col("post_id").alias("source"),
            col("city").alias("target"),
            lit("IN_CITY").alias("type"),
            struct(
                lit(current_timestamp()).alias("created_at")
            ).alias("properties")
        )
        
        # Generate INCLUDES relationships in chunks
        includes_edges = None
        for chunk in self.process_in_chunks(posts_spark):
            # First explode daily_plans to get days
            days_df = chunk.select(
                col("post_id").alias("source"),
                explode("daily_plans_parsed").alias("day_data")
            ).select(
                col("source"),
                col("day_data.day").alias("day_number"),
                explode("day_data.activities").alias("activity")
            )
            
            # Then create relationships
            chunk_includes = days_df.select(
                col("source"),
                col("activity.activity").alias("target"),
                lit("INCLUDES").alias("type"),
                struct(
                    col("day_number").alias("day"),
                    col("activity.time").alias("time"),
                    col("activity.duration").alias("duration"),
                    col("activity.location").alias("location"),
                    lit(current_timestamp()).alias("created_at")
                ).alias("properties")
            )
            
            if includes_edges is None:
                includes_edges = chunk_includes
            else:
                includes_edges = includes_edges.union(chunk_includes)
        
        # Generate LIKED relationships
        liked_edges = likes_spark.select(
            col("user_id").alias("source"),
            col("post_id").alias("target"),
            lit("LIKED").alias("type"),
            struct(
                col("like_id"),
                col("created_at")
            ).alias("properties")
        )
        
        return {
            "favorite_edges": favorite_edges,
            "created_edges": created_edges,
            "in_city_edges": in_city_edges,
            "includes_edges": includes_edges,
            "liked_edges": liked_edges
        }

def process_and_upload_to_exploitation(bucket_name, **context):
    """Process data from trusted zone and upload to exploitation zone."""
    gcs_hook = GCSHook()
    storage_client = gcs_hook.get_conn()
    bucket = storage_client.bucket(bucket_name)

    gcs_handler = capture_airflow_logs(bucket_name, context['dag'].dag_id, context)
    logger = logging.getLogger(f"airflow.task.{context['task'].task_id}")

    try:
        # Initialize Spark session
        spark = SparkSession.builder \
            .appName("Neo4j File Generator") \
            .config("spark.driver.memory", "8g") \
            .config("spark.executor.memory", "8g") \
            .config("spark.memory.offHeap.enabled", "true") \
            .config("spark.memory.offHeap.size", "4g") \
            .config("spark.sql.shuffle.partitions", "200") \
            .config("spark.default.parallelism", "200") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.sql.adaptive.skewJoin.enabled", "true") \
            .getOrCreate()

        # Get today's date in the format YYYY-MM-DD
        today = datetime.now().strftime("%Y-%m-%d")

        # Define source and destination paths
        trusted_path = f"trusted_zone/users/{today}/"
        exploitation_path = f"exploitation_zone/users/{today}/neo4j/"

        # List of files to process
        files_to_process = ['users.parquet', 'posts.parquet', 'likes.parquet']

        # Download and process each Parquet file
        dataframes = {}
        for file_name in files_to_process:
            logger.info(f"Processing {file_name}...")
            
            # Download Parquet file from trusted zone
            source_blob = bucket.blob(trusted_path + file_name)
            parquet_content = source_blob.download_as_bytes()
            
            # Read Parquet file into DataFrame
            df = pd.read_parquet(io.BytesIO(parquet_content))
            dataframes[file_name.replace('.parquet', '')] = df
            
            logger.info(f"Successfully loaded {file_name}")

        # Process the data
        users_df = dataframes['users']
        posts_df = dataframes['posts']
        likes_df = dataframes['likes']

        # Initialize Neo4j file generator
        neo4j_generator = Neo4jFileGenerator(spark)

        # Generate node files
        logger.info("Generating Neo4j node files...")
        node_files = neo4j_generator.generate_node_files(users_df, posts_df)

        # Generate edge files
        logger.info("Generating Neo4j edge files...")
        edge_files = neo4j_generator.generate_edge_files(users_df, posts_df, likes_df)

        # Convert and upload node files
        for node_type, df in node_files.items():
            # Convert to Neo4j format
            neo4j_df = df.select(
                col("id"),
                col("labels"),
                to_json(col("properties")).alias("properties")
            )
            
            # Save as CSV
            csv_buffer = io.StringIO()
            neo4j_df.toPandas().to_csv(csv_buffer, index=False)
            csv_buffer.seek(0)
            
            # Upload to GCS
            blob = bucket.blob(f"{exploitation_path}{node_type}.csv")
            blob.upload_from_string(csv_buffer.getvalue(), content_type='text/csv')
            logger.info(f"Uploaded {node_type}.csv")

        # Convert and upload edge files
        for edge_type, df in edge_files.items():
            # Convert to Neo4j format
            neo4j_df = df.select(
                col("source"),
                col("target"),
                col("type"),
                to_json(col("properties")).alias("properties")
            )
            
            # Save as CSV
            csv_buffer = io.StringIO()
            neo4j_df.toPandas().to_csv(csv_buffer, index=False)
            csv_buffer.seek(0)
            
            # Upload to GCS
            blob = bucket.blob(f"{exploitation_path}{edge_type}.csv")
            blob.upload_from_string(csv_buffer.getvalue(), content_type='text/csv')
            logger.info(f"Uploaded {edge_type}.csv")

        # Stop Spark session
        spark.stop()

        logger.info("All files processed and uploaded successfully")

    except Exception as e:
        logger.error(f"Pipeline failed: {str(e)}")
        raise
    finally:
        gcs_handler.save_logs(context)

def ingest_to_neo4j(bucket_name, **context):
    """Ingest the processed data from exploitation_zone into Neo4j."""
    gcs_hook = GCSHook()
    storage_client = gcs_hook.get_conn()
    bucket = storage_client.bucket(bucket_name)

    gcs_handler = capture_airflow_logs(bucket_name, context['dag'].dag_id, context)
    logger = logging.getLogger(f"airflow.task.{context['task'].task_id}")

    try:
        # Get today's date in the format YYYYMMDD
        today = datetime.now().strftime("%Y%m%d")

        # Define the exploitation zone path
        exploitation_prefix = f"exploitation_zone/users/{today}/neo4j/"

        # List all blobs in the exploitation zone for today
        blobs = bucket.list_blobs(prefix=exploitation_prefix)

        # Get Neo4j credentials from environment variables
        neo4j_user = os.getenv('NEO4J_USER')
        neo4j_password = os.getenv('NEO4J_PASSWORD')

        # Ingest each blob into Neo4j
        for blob in blobs:
            source_path = blob.name

            # Download the blob to a temporary file
            local_file = f"/tmp/{os.path.basename(blob.name)}"
            blob.download_to_filename(local_file)

            # Ingest the data into Neo4j using cypher-shell
            # Assuming Neo4j is running in a Docker container with the name 'neo4j'
            cypher_command = f"cat {local_file} | docker exec -i neo4j cypher-shell -u {neo4j_user} -p {neo4j_password}"
            subprocess.run(cypher_command, shell=True, check=True)
            logger.info(f"Ingested data from {source_path} into Neo4j")

            # Clean up the temporary file
            os.remove(local_file)
    except Exception as e:
        logger.error(f"Pipeline failed: {str(e)}")
        raise
    finally:
        gcs_handler.save_logs(context)

with DAG(
    'gcs_trusted_to_exploitation_users_dag',
    start_date=datetime(2025, 5, 11),
    description='DAG to process data from trusted zone and upload to exploitation zone.',
    tags=['gcs', 'users', 'exploitation'],
    schedule='@daily',
    catchup=False
) as dag:
    gcs_bucket_name = Variable.get("events_gcs_bucket_name", default_var="bdm-project")

    process_task = PythonOperator(
        task_id='process_and_upload_to_exploitation',
        python_callable=process_and_upload_to_exploitation,
        op_kwargs={'bucket_name': gcs_bucket_name}
    )

    ingest_task = PythonOperator(
        task_id='ingest_to_neo4j',
        python_callable=ingest_to_neo4j,
        op_kwargs={'bucket_name': gcs_bucket_name}
    )

    process_task >> ingest_task 
